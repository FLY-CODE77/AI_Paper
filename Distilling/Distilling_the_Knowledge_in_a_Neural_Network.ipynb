{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electric-sound",
   "metadata": {},
   "source": [
    "# Distilling_the_Knowledge_in_a_Neural_Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-validation",
   "metadata": {},
   "source": [
    "> writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-treatment",
   "metadata": {},
   "source": [
    "- Geoffrey Hinton\n",
    "- Oriol Vinyals\n",
    "- Jeff Dean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-clinic",
   "metadata": {},
   "source": [
    "# Soft targets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-encyclopedia",
   "metadata": {},
   "source": [
    "- to transfer the generalization ability of the cumbersome model to small model \n",
    "+ use the class probabilities produced by the cumbersome model \"soft target\"\n",
    "\n",
    "---\n",
    "- when the soft targets have high entrioy, \n",
    "    + they provide much more info per training case \n",
    "    + much less variance in the gradient\n",
    "    \n",
    "    - small model can be learned much faster less data and much higher learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-accent",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-catalyst",
   "metadata": {},
   "source": [
    "- raise the temperature of the final softmax until the cumbersome model \n",
    "- produces a suitably soft set of targets. \n",
    "---\n",
    "- We then use the same high temperature when training the small model to match these soft targets\n",
    "- We show later that matching the logits of the cumbersome model is actually a special case of distillation.\n",
    "\n",
    "--\n",
    "- (normal training set) if we add a small term to the objective function that encourages \n",
    "- the small model to predict the true targets as well as matching the soft targets \n",
    "- provided by the cumbersome model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-month",
   "metadata": {},
   "source": [
    "- Neural networks typically produce class probabilities by using a “softmax” \n",
    "- output layer that converts the logit, \n",
    "\n",
    "\n",
    "- $z_i$ : computed for each class \n",
    "- $q_i$ : probability\n",
    "- $z_j$ :other logits \n",
    "- T : temperature(normally set to 1)\n",
    "    + (higher value for T produces a softer probability distribution over class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-naples",
   "metadata": {},
   "source": [
    "## $q_i = \\frac{\\exp({z_i}/{T})}{\\sum_j({z_j}/{T})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-remark",
   "metadata": {},
   "source": [
    "### simplest form of distillation\n",
    "+ HOW: knowledge transferred to the distilled model\n",
    "    + by training a transfer set and using a soft targert distribution for each case \n",
    "    + in transfer set that produeced by using the cumbersome model with a high temperature in its softmax\n",
    "    + same high temperature is used whd training the distilled model \n",
    "    - but after it has been trained, it used a temperature of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-senior",
   "metadata": {},
   "source": [
    "### when the correct labels are known for all or some of transfer set\n",
    "- cumbersome model to distilled model method can significantly improved\n",
    "\n",
    "- how to do!?\n",
    "   \n",
    "   + simply use a weighted average of two different objective functions\n",
    "   \n",
    "   ---\n",
    "   + first objective functino is cross entropy with the soft targets\n",
    "     - soft targets from the cumbersome model make softmax of distilled model \n",
    "   ---\n",
    "   + second objective function is cross entropy with the correct labels\n",
    "        - computed using exactly the same logits in softmax of the distilled model\n",
    "        - but temperature is 1 \n",
    "        - the best results were generally obtained by using condiderably lower weight on 2 model\n",
    "        \n",
    "    ---    \n",
    "+ since the magnitude of gradients produced by the soft targets scaled as 1/(T^2)\n",
    "+ important to multiply them T^2 when using hard and soft targets\n",
    "- this works ensures \n",
    "- relative contribution of hard/soft targets remiain roughly unchanged \n",
    "- (when distillation temperature changed while experimenting with meta- parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-upper",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
